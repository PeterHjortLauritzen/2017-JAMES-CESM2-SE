In addition to the numerical changes described in the previous section, a number of changes to the computational structure of the SE dynamical core were also made which both reduce the computational cost at both modest and large processor count.  In particular, new communication operators were developed which reduces the amount of data movement between MPI ranks as well as through the memory hierarchy. Derivative operators were optimized to increase code vectorization, and the limiter operator was rewritten to reduce cost.  A more detailed description of the optimizations can be found in \cite{dennis2017}. The CAM-HOMME configuration is referred to as the {\em{original}} codebase and CAM-SE is referred to as the {\em{optimized}} codebase in this Section. So when comparing computational cost of CAM-HOMME and CAM-SE it reflects not only the code optimizations but also the numerous science changes in CAM-SE that require extra computational work (e.g., the heat capacity is a function of water loading tracers and therefore not constant, moist pressure is not a prognostic variable and must be diagnosed, the reference pressure computation is not in CAM-HOMME).

\begin{figure}[h]
\centering
 \includegraphics[scale=0.45]{figs/aqua-perf}
 \caption{The execution time of the SE dynamical core in an Aquaplanet configuration using CAM6 physics at $1^\circ$ horizontal resolution on Cheyenne.   Both the original (CAM-HOMME) and optimized code (CAM-SE) is indicated by the solid blue and red lines, while the dotted lines represent the computational cost.}
 \label{fig:aqua-perf}
\end{figure}

We provide the execution time and computational cost on Cheyenne for the dynamical core component for the $1^\circ$ CAM6 Aquaplanet configuration used in Section \ref{sec:APE} ($N_e=30$, $N_p=4$, $nlev=32$). Cheyenne is an SGI ICE Cluster with 4,032 dual-socket Intel Xeon based nodes with 36 cores/node   The x-axis is the number of nodes while the left y-axis corresponds to execution time in seconds/day, and the right y-axis corresponds to the relative computational cost normalized to the original code run on five nodes or 180 cores of Cheyenne. Execution times and computational costs are also provided for up to 150 nodes or 5400 cores where a single spectral element in the horizontal is allocated to each core.

It is clear from Figure \ref{fig:aqua-perf} that the execution time for the dynamical core for the optimized version (CAM-SE) is significantly less than the original codebase (CAM-HOMME) for all core counts. The reduction in execution time for the optimized versus the original code varies from approximately 20\% at small core counts to slightly more than 50\% at larger core counts. The percentage reduction in execution time for the optimized versus original code is readily apparent by looking at the relative computational cost curves indicate by the dotted lines in Figure \ref{fig:aqua-perf}.  A value greater than one indicates that it is more expensive to run a particular configuration then the original code on five nodes, while a value less indicates that it is cheaper to run a particular configuration.  This approach allows for the comparison of both the impact of the optimizations have on a particular node code as well as the impact of optimizations to code scalability.  Interestingly while the original code becomes more expensive to execution at larger core counts the optimized code actually becomes cheaper to execute at larger core counts.  In particular, the greatest reduction of 40\% in computational cost occurs on 75 nodes.  We suspect that the decrease in computational cost illustrated in Figure \ref{fig:aqua-perf} is likely due to the fact that the calculations perform in the dynamical core now fit into the Level 3 (L3) cache on 75 nodes where they did not previously in either the original code base on the optimized code base on smaller node counts.  The decreased execution time due to the calculations being L3 cache resident is sufficiently large as to overcome any increase in execution time increase due to message passing.  

The relative cost of the dynamical core for the optimized code base is compared to other pieces of the Community Atmosphere Model is illustrated in Figure \ref{fig:percent}a. We categorize four different pieces of CAM for timing purposes: physics, dynamics, I/O and the remapping of data-structures necessary between the physics and dynamics.  The faction of time that CAM spends performing the dynamics drops from a maximum of 41\% on five nodes, to a minimum of 22\% on 150 nodes.  While there is an increase in the relative cost for both the I/O and physics to dynamics interface, the largest relative increase in cost is seen in the physics calculations, that increase from 55\% of the time on five nodes to 66\% of the time on 150 nodes.  Figure \ref{fig:percent} illustrates that in CAM-SE the dynamical core is the most scalable component of the entire atmosphere model.  

Note that for this comparison we use the same Aquaplanet CAM6 configuration, which achieves simulation rates that range from a low of 2.2 Simulated years per day (SYPD) on 5 nodes, to 53.4 SYPD on 150 nodes, and run for 1 month (see Figure \ref{fig:percent}b). This length of simulation includes the default monthly history I/O output as well as the generation of a restart file. A rule of thumb for CESM is that the model should produce a minimum of 20 SYPD to be fast enough for doing 'standard' climate science with the model. CAM-SE achieves this threshold with less than 2000 processors. At this processor count CAM-FV exhibits similar throughput (not shown) and with larger processor counts CAM-SE continues to scale well.

\begin{figure}[h]
\centering
 \includegraphics[scale=0.3]{figs/perf2}
 \caption{(a) Fraction of time spent in several different sub-components of CAM for the $1^\circ$ horizontal resolution Aquaplanet simulation on Cheyenne (see text for more details). (b) Throughput in terms of simulated years per day as a function of number of nodes. The curved line is a parabolic Least-Squares fit to the data points. Note that for the right-most data-point there is only one element in the horizontal per processor (150 nodes is 5400 processors and there are $6\times N_e^2=6\times 30^2=5400$ elements in the horizontal).}
 \label{fig:percent}
\end{figure}

%       NEW
%   s:  2.24 SYPD (5 nodes  - 180 cores)
%   m:  4.18 SYPD (10 nodes - 360 cores)
%   l:  6.82 SYPD (15 nodes - 540 cores)
%  xl: 13.48 SYPD (30 nodes - 1080 cores)
% xxl: 16.66 SYPD (38 nodes - 1368 cores)
%  3x: 30.92 SYPD (75 nodes - 2700 cores)
%  4x: 53.36 SYPD (150 nodes- 5400 cores)

